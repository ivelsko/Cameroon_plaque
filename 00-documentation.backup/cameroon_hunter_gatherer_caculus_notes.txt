# to run zipit script
./005-imv-zipit.sh <filename>

# to build a malt database
#!/usr/bin/env bash
sbatch \
-c 112 \
--mem 1950000 \
--partition=supercruncher \
-o ~/slurm_logs/slurm.%j.out \
-e ~/slurm_logs/slurm.%j.err \
--mail-type=fail \
--mail-user=velsko@shh.mpg.de \
-J "MALT-Build-RSC_Pasolli2019MAGs" \
--wrap="/projects1/malt/versions/malt040/malt-build \
--step 2 \
-i /projects1/microbiome_sciences/reference_databases/refseq_Pasolli2019MAGS/refseq_genomes_bacteria_archaea_homo_complete_chromosome_scaffold_Pasolli2019MAGs_20190930.fna.gz \
-s DNA \
-d /projects1/microbiome_sciences/reference_databases/refseq_Pasolli2019MAGS/maltindexed \
-t 112 -a2taxonomy /projects1/microbiome_calculus/evolution/01-data/databases/malt/raw/nucl_acc2tax-May2017.abin"


# all Pasolli, et al. 2019 MAGs are here /projects1/microbiome_sciences/reference_databases/Pasolli2019_MAGs/representatives/
# all oral or non-western stool Pasolli, et al. 2019 MAGs are here /projects1/microbiome_sciences/reference_databases/Pasolli2019_MAGs/representatives/oral_nonwest
# cat'd into one files with 
cat *.fa > Pasolli2019MAGs.fasta
# need to combine these with the RefSeq genomes James used to make the Custom RefSeq database to the folder here
/projects1/microbiome_sciences/reference_databases/refseq/genomes/bacteria_archea_homo_20181122/refseq_genomes_bacteria_archaea_homo_complete_chromosome_scaffold_20181122.fna.gz

# as below:

#!/usr/bin/env bash
sbatch \
-c 2 \
--mem 4G \
--partition=medium \
-o ~/slurm_logs/slurm.%j.out \
-e ~/slurm_logs/slurm.%j.err \
--mail-type=fail \
--mail-user=velsko@shh.mpg.de \
-J "catmalt" \
--wrap="cat /projects1/microbiome_sciences/reference_databases/refseq/genomes/bacteria_archea_homo_20181122/refseq_genomes_bacteria_archaea_homo_complete_chromosome_scaffold_20181122.fna.gz /projects1/microbiome_sciences/reference_databases/Pasolli2019_MAGs/representatives/oral_nonwest/Pasolli2019MAGs.fasta.gz > /projects1/microbiome_sciences/reference_databases/refseq_Pasolli2019MAGS/refseq_genomes_bacteria_archaea_homo_complete_chromosome_scaffold_Pasolli2019MAGs_20191015.fna && pigz -n2 /projects1/microbiome_sciences/reference_databases/refseq_Pasolli2019MAGS/refseq_genomes_bacteria_archaea_homo_complete_chromosome_scaffold_Pasolli2019MAGs_20191015.fna"
#--wrap="zcat /projects1/microbiome_sciences/reference_databases/refseq/genomes/bacteria_archea_homo_20181122/refseq_genomes_bacteria_archaea_homo_complete_chromosome_scaffold_20181122.fna.gz | cat - /projects1/microbiome_sciences/reference_databases/Pasolli2019_MAGs/representatives/oral_nonwest/Pasolli2019MAGs.fasta > /projects1/microbiome_sciences/reference_databases/refseq_Pasolli2019MAGS/refseq_genomes_bacteria_archaea_homo_complete_chromosome_scaffold_Pasolli2019MAGs_20190930.fna && pigz -n2 /projects1/microbiome_sciences/reference_databases/refseq_Pasolli2019MAGS/refseq_genomes_bacteria_archaea_homo_complete_chromosome_scaffold_Pasolli2019MAGs_20190930.fna"

# then run the malt build script to use all for taxonomic identification
# then need to create a tree of all of the same genomes used to build this new MALT database with PhyloPhlan2 (this is not easy to install yet, the conda install isn't ready yet)
# then need to convert the tree from PhyloPhlan2 into NCBI format for MEGAN

# use this to run MALT
#!/usr/bin/env bash
sbatch \
-c 112 \
--mem 1850000 \
--partition=supercruncher \
-o ~/slurm_logs/slurm.%j.out \
-e ~/slurm_logs/slurm.%j.err \
--mail-type=fail \
--mail-type=time_limit \
--mail-user=velsko@shh.mpg.de \
-J "MaltCMCnt" \
--wrap="/projects1/microbiome_calculus/evolution/02-scripts.backup/007-malt-genbank-nt_2017_2step_85pc_supp_0.01 \
/projects1/microbiome_calculus/Cameroon_plaque/04-analysis/malt/input-temp/*.gz \
/projects1/microbiome_calculus/Cameroon_plaque/04-analysis/malt/nt-correct"

# in 02-scripts.backup
# 011-imv-maltstats.sh
# To get the mapping stats from malt runs to input into R, use the log output file and run the following code:
# malt-nt CMC only
grep "Num. of queries:" ../00-documentation.backup/malt-genbank_rma_20190903.log > ../00-documentation.backup/CMC_nt_numbers.txt
grep "Aligned queries:" ../00-documentation.backup/malt-genbank_rma_20190903.log > ../00-documentation.backup/CMC_nt_aligned.txt
ls ../04-analysis/malt/nt/*.rma6 > ../00-documentation.backup/CMC_nt_names.txt
paste ../00-documentation.backup/CMC_nt_names.txt ../00-documentation.backup/CMC_nt_numbers.txt ../00-documentation.backup/CMC_nt_aligned.txt > ../00-documentation.backup/CMC_nt_aligned_stats.tsv

# malt-RefSeqCustom CMC only
grep "Num. of queries:" ../00-documentation.backup/malt-genbank_rma_20190925.log > ../00-documentation.backup/CMCnumbers.txt
grep "Aligned queries:" ../00-documentation.backup/malt-genbank_rma_20190925.log > ../00-documentation.backup/CMCaligned.txt
ls ../04-analysis/malt/RefSeqCustom/*.rma6 | grep -v SRR | grep -v JAE > ../00-documentation.backup/CMCnames.txt
paste ../00-documentation.backup/CMCnames.txt ../00-documentation.backup/CMCnumbers.txt ../00-documentation.backup/CMCaligned.txt > ../00-documentation.backup/CMC_RSC_aligned_stats.tsv

# malt-RefSeqCustom HMP only
grep "Num. of queries:" ../00-documentation.backup/malt-genbank_rma_20190930.log > ../00-documentation.backup/HMPnumbers.txt
grep "Aligned queries:" ../00-documentation.backup/malt-genbank_rma_20190930.log > ../00-documentation.backup/HMPaligned.txt
ls ../04-analysis/malt/RefSeqCustom/SRR*.bam.rma6 > ../00-documentation.backup/HMPnames.txt
paste ../00-documentation.backup/HMPnames.txt ../00-documentation.backup/HMPnumbers.txt ../00-documentation.backup/HMPaligned.txt > ../00-documentation.backup/HMP_RSC_aligned_stats.tsv

# malt-RefSeqCustom JAE only
grep "Num. of queries:" ../00-documentation.backup/malt-genbank_rma_20191021.log > ../00-documentation.backup/JAEnumbers.txt
grep "Aligned queries:" ../00-documentation.backup/malt-genbank_rma_20191021.log > ../00-documentation.backup/JAEaligned.txt
ls ../04-analysis/malt/RefSeqCustom/JAE*.rma6 > ../00-documentation.backup/JAEnames.txt
paste ../00-documentation.backup/JAEnames.txt ../00-documentation.backup/JAEnumbers.txt ../00-documentation.backup/JAEaligned.txt > ../00-documentation.backup/JAE_RSC_aligned_stats.tsv

# malt-RefSeqCustom Yanomami only
grep "Num. of queries:" ../00-documentation.backup/malt-genbank_rma_20191218.log > ../00-documentation.backup/Yanomaminumbers.txt
grep "Aligned queries:" ../00-documentation.backup/malt-genbank_rma_20191218.log > ../00-documentation.backup/Yanomamialigned.txt
ls ../04-analysis/malt/RefSeqCustom/SRR*.truncated.rma6 > ../00-documentation.backup/Yanomaminames.txt
paste ../00-documentation.backup/Yanomaminames.txt ../00-documentation.backup/Yanomaminumbers.txt ../00-documentation.backup/Yanomamialigned.txt > ../00-documentation.backup/Yanomami_RSC_aligned_stats.tsv

# combine CMC, HMP, and JAE RefSeqCustom stats files
cat ../00-documentation.backup/CMC_RSC_aligned_stats.tsv ../00-documentation.backup/HMP_RSC_aligned_stats.tsv ../00-documentation.backup/JAE_RSC_aligned_stats.tsv ../00-documentation.backup/Yanomami_RSC_aligned_stats.tsv > ../00-documentation.backup/CMC_HMP_JAE_Yano_RSC_aligned_stats.tsv

perl -p -i -e 's/ //g' ../00-documentation.backup/CMC_nt_aligned_stats.tsv
perl -p -i -e 's/Num.ofqueries://g' ../00-documentation.backup/CMC_nt_aligned_stats.tsv
perl -p -i -e 's/Alignedqueries://g' ../00-documentation.backup/CMC_nt_aligned_stats.tsv

perl -p -i -e 's/ //g' ../00-documentation.backup/CMC_RSC_aligned_stats.tsv
perl -p -i -e 's/Num.ofqueries://g' ../00-documentation.backup/CMC_RSC_aligned_stats.tsv
perl -p -i -e 's/Alignedqueries://g' ../00-documentation.backup/CMC_RSC_aligned_stats.tsv

perl -p -i -e 's/ //g' ../00-documentation.backup/HMP_RSC_aligned_stats.tsv
perl -p -i -e 's/Num.ofqueries://g' ../00-documentation.backup/HMP_RSC_aligned_stats.tsv
perl -p -i -e 's/Alignedqueries://g' ../00-documentation.backup/HMP_RSC_aligned_stats.tsv

perl -p -i -e 's/ //g' ../00-documentation.backup/CMC_HMP_RSC_aligned_stats.tsv
perl -p -i -e 's/Num.ofqueries://g' ../00-documentation.backup/CMC_HMP_RSC_aligned_stats.tsv
perl -p -i -e 's/Alignedqueries://g' ../00-documentation.backup/CMC_HMP_RSC_aligned_stats.tsv

perl -p -i -e 's/ //g' ../00-documentation.backup/CMC_HMP_JAE_RSC_aligned_stats.tsv
perl -p -i -e 's/Num.ofqueries://g' ../00-documentation.backup/CMC_HMP_JAE_RSC_aligned_stats.tsv
perl -p -i -e 's/Alignedqueries://g' ../00-documentation.backup/CMC_HMP_JAE_RSC_aligned_stats.tsv

perl -p -i -e 's/ //g' ../00-documentation.backup/CMC_HMP_JAE_Yano_RSC_aligned_stats.tsv
perl -p -i -e 's/Num.ofqueries://g' ../00-documentation.backup/CMC_HMP_JAE_Yano_RSC_aligned_stats.tsv
perl -p -i -e 's/Alignedqueries://g' ../00-documentation.backup/CMC_HMP_JAE_Yano_RSC_aligned_stats.tsv

rm ../00-documentation.backup/*numbers.txt
rm ../00-documentation.backup/*aligned.txt
rm ../00-documentation.backup/*names.txt

# now add column titles (SampleID, Queries, Aligned) with ./add_malt_stats_headers.rb

##########################################################################################
# symlink the HMP data for Eager processing (and subsampling)
ln -s /projects1/microbiome_calculus/evolution/01-data/public_data/raw/SRR061294
ln -s /projects1/microbiome_calculus/evolution/01-data/public_data/raw/SRR062298
ln -s /projects1/microbiome_calculus/evolution/01-data/public_data/raw/SRR062299
ln -s /projects1/microbiome_calculus/evolution/01-data/public_data/raw/SRR513165
ln -s /projects1/microbiome_calculus/evolution/01-data/public_data/raw/SRR513768
ln -s /projects1/microbiome_calculus/evolution/01-data/public_data/raw/SRR513775
ln -s /projects1/microbiome_calculus/evolution/01-data/public_data/raw/SRR514202
ln -s /projects1/microbiome_calculus/evolution/01-data/public_data/raw/SRR514239
ln -s /projects1/microbiome_calculus/evolution/01-data/public_data/raw/SRR514306
ln -s /projects1/microbiome_calculus/evolution/01-data/public_data/raw/SRR514329
ln -s /projects1/microbiome_calculus/evolution/01-data/public_data/raw/SRR061192
ln -s /projects1/microbiome_calculus/evolution/01-data/public_data/raw/SRR061320
ln -s /projects1/microbiome_calculus/evolution/01-data/public_data/raw/SRR061365
ln -s /projects1/microbiome_calculus/evolution/01-data/public_data/raw/SRR061562
ln -s /projects1/microbiome_calculus/evolution/01-data/public_data/raw/SRR062083
ln -s /projects1/microbiome_calculus/evolution/01-data/public_data/raw/SRR063517
ln -s /projects1/microbiome_calculus/evolution/01-data/public_data/raw/SRR1804664
ln -s /projects1/microbiome_calculus/evolution/01-data/public_data/raw/SRR1804823
ln -s /projects1/microbiome_calculus/evolution/01-data/public_data/raw/SRR512767
ln -s /projects1/microbiome_calculus/evolution/01-data/public_data/raw/SRR513828

# subsample the HMP samples to 10000000 reads each to be more similar to the read number of the CMC samples

#!/bin/bash

#SBATCH -n 8
#SBATCH --mem 24G
#SBATCH --partition=medium
#SBATCH -o /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.out
#SBATCH -e /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.err
#SBATCH --mail-type=fail
#SBATCH --mail-type=time_limit
#SBATCH --mail-use=velsko@shh.mpg.de
#SBATCH --export=ALL
#SBATCH --array=0-19%2
#SBATCH -J "subsample"

SAMPLES=($(find /projects1/microbiome_calculus/Cameroon_plaque/03-preprocessing/human_filering/input/public_data/*/ -name '*.gz' -type f | rev | cut -d/ -f 1 | rev | cut -d_ -f 1))
SAMPLENAME=${SAMPLES[$SLURM_ARRAY_TASK_ID]}

seqtk sample -s100 /projects1/microbiome_calculus/Cameroon_plaque/03-preprocessing/human_filering/input/public_data/"$SAMPLENAME"/"$SAMPLENAME"_S0_L001_R1_000.fastq.gz 10000000 > /projects1/microbiome_calculus/Cameroon_plaque/03-preprocessing/human_filering/input/public_data/"$SAMPLENAME"10M/"$SAMPLENAME"_S0_L001_R1_000.10M.fastq
pigz -8 /projects1/microbiome_calculus/Cameroon_plaque/03-preprocessing/human_filering/input/public_data/"$SAMPLENAME"10M/"$SAMPLENAME"_S0_L001_R1_000.10M.fastq
seqtk sample -s100 /projects1/microbiome_calculus/Cameroon_plaque/03-preprocessing/human_filering/input/public_data/"$SAMPLENAME"/"$SAMPLENAME"_S0_L001_R2_000.fastq.gz 10000000 > /projects1/microbiome_calculus/Cameroon_plaque/03-preprocessing/human_filering/input/public_data/"$SAMPLENAME"10M/"$SAMPLENAME"_S0_L001_R2_000.10M.fastq
pigz -8 /projects1/microbiome_calculus/Cameroon_plaque/03-preprocessing/human_filering/input/public_data/"$SAMPLENAME"10M/"$SAMPLENAME"_S0_L001_R2_000.10M.fastq

# symlink the JAE data for Eager processing (and subsampling)
ln -s /projects1/microbiome_sciences/raw_data/internal.backup/JAE006.A0101
ln -s /projects1/microbiome_sciences/raw_data/internal.backup/JAE007.A0101
ln -s /projects1/microbiome_sciences/raw_data/internal.backup/JAE008.A0101
ln -s /projects1/microbiome_sciences/raw_data/internal.backup/JAE009.A0101
ln -s /projects1/microbiome_sciences/raw_data/internal.backup/JAE010.A0101
ln -s /projects1/microbiome_sciences/raw_data/internal.backup/JAE012.A0101
ln -s /projects1/microbiome_sciences/raw_data/internal.backup/JAE013.A0101
ln -s /projects1/microbiome_sciences/raw_data/internal.backup/JAE014.A0101
ln -s /projects1/microbiome_sciences/raw_data/internal.backup/JAE015.A0101
ln -s /projects1/microbiome_sciences/raw_data/internal.backup/JAE016.A0101

#!/bin/bash

#SBATCH -n 8
#SBATCH --mem 24G
#SBATCH --partition=medium
#SBATCH -o /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.out
#SBATCH -e /projects1/clusterhomes/velsko/slurm_logs/slurm.%j.err
#SBATCH --mail-type=fail
#SBATCH --mail-type=time_limit
#SBATCH --mail-use=velsko@shh.mpg.de
#SBATCH --export=ALL
#SBATCH --array=0-19%2
#SBATCH -J "subsample"

SAMPLES=($(find /projects1/microbiome_calculus/Cameroon_plaque/04-analysis/malt/input-temp_JAE/ -name '*.gz' -type f | rev | cut -d/ -f 1 | rev | cut -d_ -f 1))
SAMPLENAME=${SAMPLES[$SLURM_ARRAY_TASK_ID]}

# need to run conda activate py27 before running /projects1/microbiome_calculus/Cameroon_plaque/02-scripts.backup/014-imv-humann2.sh
seqtk sample -s100 /projects1/microbiome_calculus/Cameroon_plaque/04-analysis/malt/input-temp_JAE/"$SAMPLENAME"/"$SAMPLENAME"_S0_L001_R1_000.fastq.gz 10000000 > /projects1/microbiome_calculus/Cameroon_plaque/04-analysis/malt/input-temp_JAE/"$SAMPLENAME"10M/"$SAMPLENAME"_S0_L001_R1_000.10M.fastq
pigz -8 /projects1/microbiome_calculus/Cameroon_plaque/04-analysis/malt/input-temp_JAE/"$SAMPLENAME"10M/"$SAMPLENAME"_S0_L001_R1_000.10M.fastq

####
# to change the names of the HUMAnN2 folders that didn't match the basename do
rename 's/_S0_L001_R1_001.fastq.combined.fq.prefixed.extractunmapped.bam.10M.fastq.gz/.10M/' *_S0_L001_R1_001.fastq.combined.fq.prefixed.extractunmapped.bam.10M.fastq.gz
rename 's/_S0_L001_R1_000.10M.fastq.combined.fq.prefixed.extractunmapped.bam.fastq.gz/.10M/' *_S0_L001_R1_000.10M.fastq.combined.fq.prefixed.extractunmapped.bam.fastq.gz
rename 's/_S0_L001_R1_000.fastq.combined.fq.prefixed.extractunmapped.bam.fastq.gz//' *_S0_L001_R1_000.fastq.combined.fq.prefixed.extractunmapped.bam.fastq.gz
rename 's/.fastq.truncated.prefixed.gz//' *.fastq.truncated.prefixed.gz


mkdir genefamilies pathabundance pathcoverage

ls | while read line; do cp $line/*genefamilies.tsv ./genefamilies; done
ls | while read line; do cp $line/*pathabundance.tsv ./pathabundance; done
ls | while read line; do cp $line/*pathcoverage.tsv ./pathcoverage; done

# merge the output files and normalize them for direct comparison of samples within each output type
humann2_join_tables -i genefamilies -o humann2.genefamilies.all.tsv
humann2_renorm_table -i humann2.genefamilies.all.tsv -o humann2.genefamilies.all.tss.tsv -s y

humann2_join_tables -i pathabundance -o humann2.pathabundance.all.tsv
humann2_renorm_table -i humann2.pathabundance.all.tsv -o humann2.pathabundance.all.tss.tsv -s n

humann2_join_tables -i pathcoverage -o humann2.pathcoverage.all.tsv
humann2_renorm_table -i humann2.pathcoverage.all.tsv -o humann2.pathcoverage.all.tss.tsv -s n

mkdir genefamilies/renormed
for f in ./genefamilies/*.tsv; do humann2_renorm_table -i $f -o ./genefamilies/renormed/$(basename $f .tsv).tss.tsv; done

# download databases for regrouping entries
humann2_databases --download utility_mapping full /projects1/users/velsko/bin/humann2_regroup_databases

humann2_join_tables -i ./genefamilies/renormed -o ./humann2.genefamilies.all.i.tss.tsv
humann2_regroup_table --input humann2.genefamilies.all.i.tss.tsv --output ./humann2.genefamilies.all.i.tss.ko.tsv --groups uniref90_ko
humann2_renorm_table -i ./humann2.genefamilies.all.i.tss.ko.tsv -o ./humann2.genefamilies.all.i.tss.renorm.ko.tsv -s y

# To determine the # of reads that were aligned (or rather unaligned) by HUMAnN2
# working dir /projects1/microbiome_calculus/Cameroon_plaque/04-analysis/humann2/input
ls *.gz > samples.list
touch samples.lines
for f in *.gz; do zcat $f | wc -l >> samples.lines; done
# add a header to each file (SampleID, Lines)
paste samples.list samples.lines > ../../../05-results.backup/sample_lines.tsv 
# check that these numbers, divided by 4, are the same as those in the metadata file column Individual_Seq_Depth_NonHuman
# if they are, use that metadata column for the R analyses, otherwise figure out why that column is wrong and fix it to match these. Use it with the correct numbers

cd ../logfiles/
touch files.names unaligned.pcts
for f in *.out; do grep "Creating output" $f | sed 's/Creating\ output\ directory\:\ \/projects1\/microbiome_calculus\/Cameroon_plaque\/04-analysis\/humann2\/output\///g' >> files.names; done
for f in *.out; do grep "Unaligned reads after translated alignment:" $f | sed 's/Unaligned\ reads\ after\ translated\ alignment\://g' | sed 's/\ \%//g' >> unaligned.pcts; done
# add a header to each file (SampleID, UnalignedPct)
paste files.names unaligned.pcts > ../../../05-results.backup/humann2_alignment_stats.tsv


###########################
run kraken 5X with both databases to see if there are more reads assigned when including MAGs, account for variability with the 5 runs
# get the number of classified and unclassified reads for each sample for each run with each database to compare 
# if adding the Pasolli MAGs to the database increases the number of reads classified in each sample

# **Note** there was no difference in the number of reads assigned in each sample between the 5 runs, so when running again use only 3 runs

# the MetaPhlAn2-type output may be too difficult to work with, so re-run with 3 runs each using the standard output format

# **Note** there is a problem with Kraken2, where it will call reads classified 'C' but have them at rank '0'
# in the report file then, the number of unclassified + root reads doesn't equal the total. The missing reads are those above. 
# These reads are not included in the report file, but are included in the output file. Apparently the developers are aware of the problem and working on it

# run this for both outputs (RefSeqOnly, RefSeqPasolliMAGs). Add .rso to the files that had the RefSeqOnly database
ls *.output.* > sampleorder.rso.list
ls *.output.* > sampleorder.rspm.list

touch classifiedreads.rso.counts
for f in *.output.*; do grep -c "^C" $f >> classifiedreads.rso.counts; done
touch unclassifiedreads.rso.counts
for f in *.output.*; do grep -c "^U" $f >> unclassifiedreads.rso.counts; done

# open each file (sampleorder.list, classifiedreads.counts, unclassifiedreads.counts) and give each a header
# SampleID, Classified, Unclassified, respectively

paste sampleorder.rspm.list classifiedreads.rspm.counts unclassifiedreads.rspm.counts > readclassification_stats.rspm.tsv
paste sampleorder.rso.list classifiedreads.rso.counts unclassifiedreads.rso.counts > readclassification_stats.rso.tsv
cp readclassification_stats.rspm.tsv /projects1/microbiome_calculus/Cameroon_plaque/05-results.backup/readclassification_stats.rspm.tsv
cp readclassification_stats.rso.tsv /projects1/microbiome_calculus/Cameroon_plaque/05-results.backup/readclassification_stats.rso.tsv

# Now load these files (readclassification_stats.tsv, readclassification_stats.rso.tsv) into R to compare the number of classified reads. Is it higher when using the database with the Pasolli MAGs?

# to get taxonomy for the standard report files:
# in /projects1/microbiome_calculus/Cameroon_plaque/04-analysis/kraken/output/mpa_output/RefSeqOnly/no_header
# in /projects1/microbiome_calculus/Cameroon_plaque/04-analysis/kraken/output/mpa_output/RefSeqPasolliMAGs/no_header

touch taxonomy.list
for f in *.old
do
awk -F"\t" '{print $1}' $f >> taxonomy.list
done
sort taxonomy.list | uniq > taxonomy.list.uniq


touch taxonomy.list
for f in *.old
do
awk -F"\t" '{print $1}' $f >> taxonomy.list
done
sort taxonomy.list | uniq > taxonomy.list.uniq

awk -F"\|" '{print $NF}' taxonomy.list.uniq | sed 's/d__//g' | sed 's/p__//g' | sed 's/c__//g' | sed 's/o__//g' | sed 's/f__//g' | sed 's/g__//g' | sed 's/s__//g' | paste - taxonomy.list.uniq > taxonomy.list.uniq.split
cat /projects1/microbiome_calculus/Cameroon_plaque/04-analysis/kraken/output/mpa_output/RefSeqPasolliMAGs/no_header/taxonomy.list.uniq.split /projects1/microbiome_calculus/Cameroon_plaque/04-analysis/kraken/output/mpa_output/RefSeqOnly/no_header/taxonomy.list.uniq.split | sort | uniq > /projects1/microbiome_calculus/Cameroon_plaque/00-documentation.backup/taxonomy.list.uniq.split

# open file and add column names SciName and Taxonomy

# open taxonomy_na.tsv and add in the appropriate taxonomy

###########################
To download oral bacteriophage sequences from MG-RAST

wget ftp://ftp.metagenomics.anl.gov/tools/download/mg-download.py
chmod u+x mg-download.py

source activate 2.7
mg-download.py --project mgp7236 --dir /projects1/microbiome_sciences/raw_data/public/abeles2014

# run the Abeles 2014 data through FastQC to see if there are adapters that need removing
TACGAGTATGCCTACGGGAGGCAGCAGTAGGGAATCTTCGGCAATGGACG	
CGTAGACTAGCCTACGGGAGGCAGCAGTGAGGAATATTGGTCAATGGGCG
ACGACTACAGCCTACGGGAGGCAGCAGTAGGGAATCTTCGGCAATGGACG
ACGAGTGCGTCCTACGGGAGGCAGCAGTAGGGAATCTTCGGCAATGGACG


# try running the reads through cutadapt to match the parameters that were used by Abeles, et al. 2014
cutadapt -a GCCTACGGGAGGCAGCAGTAGGGAATCTTCGGCAATGGACG -m 50 -M 300 --max-n 0.25 -j <NumberOfCores> -o <output.fastq.gz> <input.fastq.gz>
cutadapt -u 10 -o trimmed.fastq reads.fastq # to cut off 10 bases from the begining


# assemble the phage reads with SPAdes
sbatch 018-assemblephage.sh

# check assemblies with quast (or metaquast?)
019-imv-quast.sh /projects1/microbiome_calculus/Cameroon_plaque/01-data/phage/abeles2014assemblies/SPAdes/scaffolds.list


# index the phage sequences from the assemblies in /projects1/microbiome_calculus/Cameroon_plaque/01-data/databases
bwa index -p abeles2014phage /projects1/microbiome_calculus/Cameroon_plaque/01-data/databases/abeles2014phage.fastq.gz




find /projects1/microbiome_sciences/raw_data/public/abeles2014/ -name '*.gz' -type f | rev | cut -d/ -f 1 | rev | cut -d. -f 1 | awk '{print "/projects1/microbiome_calculus/Cameroon_plaque/01-data/phage/abeles2014assemblies/SPAdes/"$1"/scaffolds.fasta"}' > contig.list


find /projects1/microbiome_calculus/Cameroon_plaque/01-data/phage/abeles2014assemblies/SPAdes/*/ -name '*ds.fasta' -type f | rev | cut -d/ -f 2 | rev | cut -d. -f1 | grep -v K | grep -v misc

########################################################

# assemble the data

nextflow run nf-core/mag \
--reads '/projects1/microbiome_calculus/Cameroon_plaque/04-analysis/assembly/input/*.R{1,2}.fastq.gz' \
-profile shh \
--kraken2_db 'ftp://ftp.ccb.jhu.edu/pub/data/kraken2_dbs/minikraken2_v2_8GB_201904_UPDATE.tgz'  \
--outdir '/projects1/microbiome_calculus/Cameroon_plaque/04-analysis/assembly/output' \
-name 'cmc_assembly' \
-w '/projects1/microbiome_calculus/Cameroon_plaque/04-analysis/assembly/output/work'

# this is the database Alex made, but it doesn't work with this pipeline
# --kraken2_db '/projects1/microbiome_sciences/reference_databases/refseq20191017_Pasolli2019/kraken2_db/for_nf-core-mag/MiniKraken_RefSeq1910PlusPasolliSGBs.tar.gz'  \


# to resume a stalled run after deleting files
nextflow run nf-core/mag --reads '/projects1/microbiome_calculus/Cameroon_plaque/04-analysis/assembly/input/*.R{1,2}.fastq.gz' -profile shh --kraken2_db 'ftp://ftp.ccb.jhu.edu/pub/data/kraken2_dbs/minikraken2_v2_8GB_201904_UPDATE.tgz'  --outdir '/projects1/microbiome_calculus/Cameroon_plaque/04-analysis/assembly/output' -w '/projects1/microbiome_calculus/Cameroon_plaque/04-analysis/assembly/output/work' -resume cmc_assembly


ERROR ~ Error executing process > 'quast (MEGAHIT-LIB050.A0105.SG1.1)'

Caused by:
  Process `quast (MEGAHIT-LIB050.A0105.SG1.1)` terminated with an error exit status (4)

Command executed:

  metaquast.py --threads "1" --rna-finding --max-ref-number 0 -l "MEGAHIT-LIB050.A0105.SG1.1" "LIB050.A0105.SG1.1.contigs.fa" -o "LIB050.A0105.SG1.1_QC"

Command exit status:
  4

Command output:
  /opt/conda/envs/nf-core-mag-1.0.0/lib/python3.6/site-packages/quast-5.0.2-py3.6.egg-info/scripts/metaquast.py --threads 1 --rna-finding --max-ref-number 0 -l MEGAHIT-LIB050.A0105.SG1.1 LIB050.A0105.
SG1.1.contigs.fa -o LIB050.A0105.SG1.1_QC

  Version: 5.0.2

  System information:
    OS: Linux-4.4.0-38-generic-x86_64-with-debian-9.9 (linux_64)                                                                                                                              [23/12529]
    Python version: 3.6.7
    CPUs number: 64

  Started: 2020-02-19 13:07:10

  Logging to LIB050.A0105.SG1.1_QC/metaquast.log

  Contigs:
    Pre-processing...
  WARNING: Skipping MEGAHIT-LIB050.A0105.SG1.1 because it doesn't contain contigs >= 0 bp.


  ERROR! None of the assembly files contains correct contigs. Please, provide different files or decrease --min-contig threshold.

Command wrapper:
  /opt/conda/envs/nf-core-mag-1.0.0/lib/python3.6/site-packages/quast-5.0.2-py3.6.egg-info/scripts/metaquast.py --threads 1 --rna-finding --max-ref-number 0 -l MEGAHIT-LIB050.A0105.SG1.1 LIB050.A0105.
SG1.1.contigs.fa -o LIB050.A0105.SG1.1_QC

  Version: 5.0.2
    System information:                                                                                                                                                                          [2/12529]
    OS: Linux-4.4.0-38-generic-x86_64-with-debian-9.9 (linux_64)
    Python version: 3.6.7
    CPUs number: 64

  Started: 2020-02-19 13:07:10

  Logging to LIB050.A0105.SG1.1_QC/metaquast.log

  Contigs:
    Pre-processing...
  WARNING: Skipping MEGAHIT-LIB050.A0105.SG1.1 because it doesn't contain contigs >= 0 bp.


  ERROR! None of the assembly files contains correct contigs. Please, provide different files or decrease --min-contig threshold.

Work dir:
  /projects1/microbiome_calculus/Cameroon_plaque/04-analysis/assembly/output/work/3f/ff67506920c98fbed4f39ec6eda583

Tip: you can replicate the issue by changing to the process work dir and entering the command `bash .command.run`
 -- Check '.nextflow.log' file for details


---
ERROR ~ Error executing process > 'metabat (MEGAHIT-CMC020.A0101.SG1.1)'                                                                                                                                                                                               [69/6275]

Caused by:
  Process `metabat (MEGAHIT-CMC020.A0101.SG1.1)` terminated with an error exit status (1)

Command executed:

  jgi_summarize_bam_contig_depths --outputDepth depth.txt MEGAHIT-CMC020.A0101.SG1.1-CMC038.A0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC008.A0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC043.B0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC045.A0201.SG1.1.bam MEGAHIT-CMC020
.A0101.SG1.1-CMC009.A0202.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC002.A0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-EXB059.A1701.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC041.A0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC017.A0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC005.A0
202.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC035.B0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC011.A0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC039.B0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC026.A0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-EXB059.A1401.SG1.1.bam MEGAHIT-
CMC020.A0101.SG1.1-CMC028.B0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC024.B0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC003.A0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-EXB059.A2001.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC045.B0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC
004.B0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC014.A0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC029.A0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC028.A0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-EXB059.A1001.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC013.B0101.SG1.1.bam ME
GAHIT-CMC020.A0101.SG1.1-CMC015.A0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC032.A0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC013.A0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC025.A0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC020.B0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1
.1-CMC034.B0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC003.B0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC031.A0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC029.B0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC033.B0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC007.B0101.SG1.1.
bam MEGAHIT-CMC020.A0101.SG1.1-CMC021.A0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-EXB059.A1201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC027.B0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC002.B0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-EXB059.A1801.SG1.1.bam MEGAHIT-CMC020.A01
01.SG1.1-CMC012.A0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC036.A0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC017.B0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-EXB059.A1501.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC021.B0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC004.A0101.
SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC006.A0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC023.B0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-EXB059.A1101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC042.A0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC018.A0101.SG1.1.bam MEGAHIT-CMC0
20.A0101.SG1.1-CMC023.A0301.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC040.B0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC009.B0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC037.B0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC024.A0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC027.
A0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC022.B0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC044.A0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC006.B0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-EXB059.A1901.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC010.A0101.SG1.1.bam MEGAHI
T-CMC020.A0101.SG1.1-CMC046.B0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC030.A0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC034.A0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC037.A0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC008.B0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-E
XB034.A2101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC010.B0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC030.B0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC026.B0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC038.B0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC022.A0101.SG1.1.bam
MEGAHIT-CMC020.A0101.SG1.1-EXB034.A2201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC047.A0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC005.B0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC001.B0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC042.B0101.SG1.1.bam MEGAHIT-CMC020.A0101.S
G1.1-CMC012.B0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC036.B0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC033.A0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-EXB059.A0901.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC018.B0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC031.B0201.SG1.
1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC043.A0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC035.A0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC020.A0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-EXB059.A1301.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC001.A0201.SG1.1.bam MEGAHIT-CMC020.A
0101.SG1.1-CMC015.B0201.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC016.A0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC001.A0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC011.B0101.SG1.1.bam MEGAHIT-CMC020.A0101.SG1.1-CMC016.A0102.SG1.1.bam
  metabat2 -t "8" -i "CMC037.A0201.SG1.1.contigs.fa" -a depth.txt -o "MetaBAT2/MEGAHIT-CMC020.A0101.SG1.1" -m 1500

  #if bin folder is empty
    if [ -z "$(ls -A MetaBAT2)" ]; then                                                                                                                                                                                                                                  [41/6275]
      cp CMC037.A0201.SG1.1.contigs.fa MetaBAT2/MEGAHIT-CMC037.A0201.SG1.1.contigs.fa
  fi

Command exit status:
  1

Command output:
  MetaBAT 2 (v2.13 (Bioconda)) using minContig 1500, minCV 1.0, minCVSum 1.0, maxP 95%, minS 60, and maxEdges 200.

Command error:
  Thread 54 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC040.B0101.SG1.1.bam with 7645438 reads and 357577 readsWellMapped
  Thread 20 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC004.B0201.SG1.1.bam with 11766672 reads and 457544 readsWellMapped
  Thread 9 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC005.A0202.SG1.1.bam with 6390988 reads and 518183 readsWellMapped
  Thread 39 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC027.B0201.SG1.1.bam with 8076754 reads and 376963 readsWellMapped
  Thread 21 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC031.B0201.SG1.1.bam with 8074134 reads and 490453 readsWellMapped
  Thread 59 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC022.B0101.SG1.1.bam with 8137804 reads and 1004069 readsWellMapped
  Thread 29 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC025.A0201.SG1.1.bam with 7967768 reads and 496110 readsWellMapped
  Thread 17 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC003.A0201.SG1.1.bam with 7437302 reads and 1180792 readsWellMapped
  Thread 22 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC029.A0201.SG1.1.bam with 7536658 reads and 686848 readsWellMapped
  Thread 51 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC042.A0201.SG1.1.bam with 7224804 reads and 1038349 readsWellMapped
  Thread 0 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC038.A0101.SG1.1.bam with 6791180 reads and 733465 readsWellMapped
  Thread 11 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC011.A0201.SG1.1.bam with 8498700 reads and 336136 readsWellMapped
  Thread 7 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC041.A0201.SG1.1.bam with 11334368 reads and 473293 readsWellMapped
  Thread 26 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC015.A0201.SG1.1.bam with 6192152 reads and 631735 readsWellMapped
  Thread 25 finished: MEGAHIT-CMC020.A0101.SG1.1-EXB059.A1301.SG1.1.bam with 3673476 reads and 213768 readsWellMapped
  Thread 37 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC021.A0201.SG1.1.bam with 7438364 reads and 614693 readsWellMapped
  Thread 11 finished: MEGAHIT-CMC020.A0101.SG1.1-EXB034.A2201.SG1.1.bam with 820252 reads and 2428 readsWellMapped
  Thread 18 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC033.A0201.SG1.1.bam with 8802280 reads and 1204864 readsWellMapped                                                                                                                                                 [13/6275]
  Thread 16 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC024.B0101.SG1.1.bam with 6540908 reads and 1893324 readsWellMapped
  Thread 19 finished: MEGAHIT-CMC020.A0101.SG1.1-EXB059.A0901.SG1.1.bam with 2095528 reads and 40409 readsWellMapped
  Thread 13 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC026.A0101.SG1.1.bam with 8796936 reads and 1832150 readsWellMapped
  Thread 23 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC028.A0101.SG1.1.bam with 9362664 reads and 2001331 readsWellMapped
  Thread 27 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC015.B0201.SG1.1.bam with 7593292 reads and 303748 readsWellMapped
  Thread 17 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC036.B0101.SG1.1.bam with 8026456 reads and 520254 readsWellMapped
  Thread 8 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC026.B0101.SG1.1.bam with 7804518 reads and 417766 readsWellMapped
  Thread 22 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC043.A0101.SG1.1.bam with 7228028 reads and 537513 readsWellMapped
  Thread 3 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC037.A0201.SG1.1.bam with 8001852 reads and 1446956 readsWellMapped
  Thread 28 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC016.A0101.SG1.1.bam with 8127778 reads and 267755 readsWellMapped
  Thread 31 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC016.A0102.SG1.1.bam with 8120824 reads and 1087898 readsWellMapped
  Thread 24 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC020.A0101.SG1.1.bam with 9034080 reads and 4280859 readsWellMapped
  Thread 30 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC011.B0101.SG1.1.bam with 7957358 reads and 342289 readsWellMapped
  Thread 7 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC030.B0101.SG1.1.bam with 7574400 reads and 178496 readsWellMapped
  Thread 2 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC034.A0101.SG1.1.bam with 8084606 reads and 1085627 readsWellMapped
  Thread 13 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC005.B0101.SG1.1.bam with 9313206 reads and 375963 readsWellMapped
  Thread 0 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC046.B0201.SG1.1.bam with 8512550 reads and 552568 readsWellMapped
  Thread 1 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC030.A0101.SG1.1.bam with 8379802 reads and 1140573 readsWellMapped
  Thread 29 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC001.A0101.SG1.1.bam with 5967376 reads and 737066 readsWellMapped
  Thread 15 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC042.B0101.SG1.1.bam with 8343362 reads and 682696 readsWellMapped
  Thread 9 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC038.B0101.SG1.1.bam with 7264058 reads and 397520 readsWellMapped
  Thread 4 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC008.B0201.SG1.1.bam with 7313902 reads and 1029105 readsWellMapped
  Thread 10 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC022.A0101.SG1.1.bam with 7381068 reads and 1986468 readsWellMapped
  Thread 20 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC018.B0101.SG1.1.bam with 7540052 reads and 402545 readsWellMapped
  Thread 16 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC012.B0201.SG1.1.bam with 6553870 reads and 343946 readsWellMapped
  Thread 26 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC001.A0201.SG1.1.bam with 8997532 reads and 1176179 readsWellMapped
  Thread 23 finished: MEGAHIT-CMC020.A0101.SG1.1-CMC035.A0201.SG1.1.bam with 10371284 reads and 1685574 readsWellMapped
  Creating depth matrix file: depth.txt
  Closing most bam files
  Closing last bam file
  Finished
  [Error!] the order of contigs in abundance file is not the same as the assembly file: k141_7383

Work dir:
  /projects1/microbiome_calculus/Cameroon_plaque/04-analysis/assembly/output/work/20/eb476fddcd5d1b6b8d9833ca584193

Tip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`

 -- Check '.nextflow.log' file for details






velsko@mpi-sdag1:/projects1/microbiome_calculus/Cameroon_plaque/04-analysis/aadder/output/Nov2018acc$ nextflow run nf-core/mag --reads '/projects1/microbiome_calculus/Cameroon_plaque/04-analysis/assembly/input/*.R{1,2}.fastq.gz' -profile shh --kraken2_db 'ftp://ftp.ccb.jh
u.edu/pub/data/kraken2_dbs/minikraken2_v2_8GB_201904_UPDATE.tgz'  --outdir '/projects1/microbiome_calculus/Cameroon_plaque/04-analysis/assembly/output' -w '/projects1/microbiome_calculus/Cameroon_plaque/04-analysis/assembly/output/work' -resume cmc_assembly
N E X T F L O W  ~  version 19.04.0
Launching `nf-core/mag` [tiny_ekeblad] - revision: 4c2f61cbbb [master]
WARN: It appears you have never run this project before -- Option `-resume` is ignored
WARN: Access to undefined parameter `readPaths` -- Initialise it to a default value eg. `params.readPaths = some_value`
WARN: Access to undefined parameter `fasta` -- Initialise it to a default value eg. `params.fasta = some_value`
Pipeline Release  : master
Run Name          : tiny_ekeblad
Reads             : /projects1/microbiome_calculus/Cameroon_plaque/04-analysis/assembly/input/*.R{1,2}.fastq.gz
Fasta Ref         : null
Data Type         : Paired-End
Kraken2 Db        : ftp://ftp.ccb.jhu.edu/pub/data/kraken2_dbs/minikraken2_v2_8GB_201904_UPDATE.tgz
Busco Reference   : https://busco-archive.ezlab.org/v3/datasets/bacteria_odb9.tar.gz
Max Resources     : 256 GB memory, 32 cpus, 24d 20h 31m 24s time per job
Container         : singularity - nfcore/mag:1.0.0
Output dir        : /projects1/microbiome_calculus/Cameroon_plaque/04-analysis/assembly/output
Launch dir        : /projects1/microbiome_calculus/Cameroon_plaque/04-analysis/aadder/output/Nov2018acc
Working dir       : /projects1/microbiome_calculus/Cameroon_plaque/04-analysis/assembly/output/work
Script dir        : /projects1/clusterhomes/velsko/.nextflow/assets/nf-core/mag
User              : velsko
Config Profile    : shh
Config Description: Generic MPI-SHH cluster(s) profile provided by nf-core/configs.
Config Contact    : James Fellows Yates (@jfy133), Maxime Borry (@Maxibor)
Config URL        : https://shh.mpg.de
executor >  slurm (77)                                                                                                                                                                                                                                                 [59/6648]
executor >  slurm (77)
executor >  slurm (77)
executor >  slurm (77)
executor >  slurm (77)
executor >  slurm (77)
[64/f2cbd8] process > phix_download_db       [100%] 1 of 1 ✔
[7a/3ac69f] process > fastp                  [100%] 39 of 39
[de/73cec0] process > fastqc_raw             [100%] 36 of 36
[f0/82d032] process > get_software_versions  [100%] 1 of 1 ✔
[8a/3b3835] process > kraken2_db_preparation [  0%] 1 of 0, failed: 1
ERROR ~ Error executing process > 'kraken2_db_preparation (1)'

Caused by:
  Can't stage file ftp://ftp.ccb.jhu.edu/pub/data/kraken2_dbs/minikraken2_v2_8GB_201904_UPDATE.tgz -- reason: pub/data/kraken2_dbs/minikraken2_v2_8GB_201904_UPDATE.tgz

Source block:
  """
  tar -xf "${db}"
  """

Work dir:
  /projects1/microbiome_calculus/Cameroon_plaque/04-analysis/assembly/output/work/8a/3b383522700f7823f76790e4b8bccd

Tip: view the complete command output by changing to the process work dir and entering the command `cat .command.out`

 -- Check '.nextflow.log' file for details
 
 
 
 
velsko@mpi-sdag1:/projects1/microbiome_calculus/Cameroon_plaque/04-analysis/aadder/output/Nov2018acc$ nextflow run nf-core/mag --reads '/projects1/microbiome_calculus/Cameroon_plaque/04-analysis/assembly/input/*.R{1,2}.fastq.gz' -profile shh --kraken2_db 'ftp://ftp.ccb.jh
u.edu/pub/data/kraken2_dbs/old/minikraken2_v2_8GB_201904_UPDATE.tgz'  --outdir '/projects1/microbiome_calculus/Cameroon_plaque/04-analysis/assembly/output' -w '/projects1/microbiome_calculus/Cameroon_plaque/04-analysis/assembly/output/work' -resume cmc_assembly
N E X T F L O W  ~  version 19.04.0
Launching `nf-core/mag` [reverent_dubinsky] - revision: 4c2f61cbbb [master]
WARN: Access to undefined parameter `readPaths` -- Initialise it to a default value eg. `params.readPaths = some_value`
WARN: Access to undefined parameter `fasta` -- Initialise it to a default value eg. `params.fasta = some_value`
Pipeline Release  : master
Run Name          : reverent_dubinsky
Reads             : /projects1/microbiome_calculus/Cameroon_plaque/04-analysis/assembly/input/*.R{1,2}.fastq.gz
Fasta Ref         : null
Data Type         : Paired-End
Kraken2 Db        : ftp://ftp.ccb.jhu.edu/pub/data/kraken2_dbs/old/minikraken2_v2_8GB_201904_UPDATE.tgz
Busco Reference   : https://busco-archive.ezlab.org/v3/datasets/bacteria_odb9.tar.gz
Max Resources     : 256 GB memory, 32 cpus, 24d 20h 31m 24s time per job
Container         : singularity - nfcore/mag:1.0.0
Output dir        : /projects1/microbiome_calculus/Cameroon_plaque/04-analysis/assembly/output
Launch dir        : /projects1/microbiome_calculus/Cameroon_plaque/04-analysis/aadder/output/Nov2018acc
Working dir       : /projects1/microbiome_calculus/Cameroon_plaque/04-analysis/assembly/output/work
Script dir        : /projects1/clusterhomes/velsko/.nextflow/assets/nf-core/mag
User              : velsko
Config Profile    : shh
Config Description: Generic MPI-SHH cluster(s) profile provided by nf-core/configs.
Config Contact    : James Fellows Yates (@jfy133), Maxime Borry (@Maxibor)
Config URL        : https://shh.mpg.de
executor >  slurm (2)
[c0/ad731a] process > fastqc_raw             [ 95%] 36 of 38, cached: 36
executor >  slurm (2)
[c0/ad731a] process > fastqc_raw             [100%] 38 of 38, cached: 36, failed: 2
executor >  slurm (2)
[c0/ad731a] process > fastqc_raw             [100%] 38 of 38, cached: 36, failed: 2
[64/465396] process > fastp                  [100%] 39 of 39, cached: 39
[f0/82d032] process > get_software_versions  [100%] 1 of 1, cached: 1 ✔
[64/f2cbd8] process > phix_download_db       [100%] 1 of 1, cached: 1 ✔
[a5/7d8194] process > kraken2_db_preparation [  0%] 1 of 0, failed: 1
Staging foreign file: ftp://ftp.ccb.jhu.edu/pub/data/kraken2_dbs/old/minikraken2_v2_8GB_201904_UPDATE.tgz
Execution cancelled -- Finishing pending tasks before exit
WARN: Unable to stage foreign file: ftp://ftp.ccb.jhu.edu/pub/data/kraken2_dbs/old/minikraken2_v2_8GB_201904_UPDATE.tgz (try 1) -- Cause: pub/data/kraken2_dbs/old/minikraken2_v2_8GB_201904_UPDATE.tgz
WARN: Unable to stage foreign file: ftp://ftp.ccb.jhu.edu/pub/data/kraken2_dbs/old/minikraken2_v2_8GB_201904_UPDATE.tgz (try 2) -- Cause: pub/data/kraken2_dbs/old/minikraken2_v2_8GB_201904_UPDATE.tgz
WARN: Unable to stage foreign file: ftp://ftp.ccb.jhu.edu/pub/data/kraken2_dbs/old/minikraken2_v2_8GB_201904_UPDATE.tgz (try 3) -- Cause: pub/data/kraken2_dbs/old/minikraken2_v2_8GB_201904_UPDATE.tgz
WARN: Killing pending tasks (2)
ERROR ~ Error executing process > 'kraken2_db_preparation (1)'

Caused by:
  Can't stage file ftp://ftp.ccb.jhu.edu/pub/data/kraken2_dbs/old/minikraken2_v2_8GB_201904_UPDATE.tgz -- reason: pub/data/kraken2_dbs/old/minikraken2_v2_8GB_201904_UPDATE.tgz

Source block:
  """
  tar -xf "${db}"
  """

Work dir:
  /projects1/microbiome_calculus/Cameroon_plaque/04-analysis/assembly/output/work/a5/7d8194ef6a450faa240eac1779a5ca

Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`

 -- Check '.nextflow.log' file for details

########################################################

snakemake -s aadder.Snakefile --cluster-config aadder-cluster.config --cluster 'sbatch --mem {cluster.mem} -p {cluster.partition} -o {cluster.out} -e {cluster.err} -n {threads}' -j 1 # -n # for a test run

snakemake -s strepSRRP_gene_align_cluster.Snakefile --cluster-config decipher-cluster.config --cluster 'sbatch --mem {cluster.mem} -p {cluster.partition} -o {cluster.out} -e {cluster.err} -n {threads}' -j -n # for a test run

########################################################
# Taxonomy differences between Kraken assignemnts with RefSeqOnly and RefSeq+PasolliMAGs
# pre-MetacodeR file fixes
# fixing the taxonomy names in 00-documentation.backup/all_taxonomy_filtered_hand.csv
sed 's/VIruses/Viruses/g' all_taxonomy_filtered_hand.csv > all_taxonomy_filtered_hand-fixed.csv
sed 's/root\|d__Viruses/root\|_d__Viruses/g' all_taxonomy_filtered_hand-fixed.csv > all_taxonomy_filtered_hand-fixed2.csv
rm all_taxonomy_filtered_hand-fixed.csv
mv all_taxonomy_filtered_hand-fixed2.csv all_taxonomy_filtered_hand.csv
sed 's/root\|d__Eukaryota/root\|_d__Eukaryota/g' all_taxonomy_filtered_hand.csv > all_taxonomy_filtered_hand-fixed2.csv
sed 's/Eukaryotas/Eukaryota/g' all_taxonomy_filtered_hand-fixed2.csv > all_taxonomy_filtered_hand-fixed3.csv
rm all_taxonomy_filtered_hand-fixed2.csv
mv all_taxonomy_filtered_hand-fixed3.csv all_taxonomy_filtered_hand.csv
sed 's/d__Eukaryota/d__Fungi/g' all_taxonomy_filtered_hand.csv > all_taxonomy_filtered_hand-fixed2.csv
mv all_taxonomy_filtered_hand-fixed2.csv all_taxonomy_filtered_hand.csv
sed 's/root\|d__Fungi/root\|_d__Fungi/g' all_taxonomy_filtered_hand.csv > all_taxonomy_filtered_hand-fixed2.csv
mv all_taxonomy_filtered_hand-fixed2.csv all_taxonomy_filtered_hand.csv
sed 's/\|k__Fungi//g' all_taxonomy_filtered_hand.csv > all_taxonomy_filtered_hand-fixed2.csv
mv all_taxonomy_filtered_hand-fixed2.csv all_taxonomy_filtered_hand.csv
sed 's/root\|d__Bacteria/root\|_d__Bacteria/g' all_taxonomy_filtered_hand.csv > all_taxonomy_filtered_hand-fixed2.csv
mv all_taxonomy_filtered_hand-fixed2.csv all_taxonomy_filtered_hand.csv
sed 's/root\|d__Archaea/root\|_d__Archaea/g' all_taxonomy_filtered_hand.csv > all_taxonomy_filtered_hand-fixed2.csv
mv all_taxonomy_filtered_hand-fixed2.csv all_taxonomy_filtered_hand.csv












